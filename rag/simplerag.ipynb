{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcc3be26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'simple.txt'}, page_content='Excellent question ðŸš€ This is the **heart of RAG (Retrieval Augmented Generation)**.\\nLetâ€™s break it down step by step so itâ€™s crystal clear:\\n\\n---\\n\\n## ðŸ”¹ 1. What is a Retriever in RAG?\\n\\nIn **RAG (Retrieval-Augmented Generation)**, the LLM doesnâ€™t just generate text blindly â€” it **retrieves relevant documents** from a knowledge base first, then uses them to answer.\\n\\nThe component that **fetches relevant documents** from a vector store (like FAISS, Pinecone, Weaviate, Chroma) is called a **Retriever**.\\n\\nðŸ‘‰ In LangChain, **retrievers** are a **standardized interface** around search methods (similarity search, Max Marginal Relevance, hybrid search, etc.).\\n\\n---\\n\\n## ðŸ”¹ 2. Similarity Search vs Retriever\\n\\n### **(a) Similarity Search**\\n\\n* A low-level method.\\n* You directly ask your **vector database**:\\n\\n  ```python\\n  results = vectorstore.similarity_search(query, k=3)\\n  ```\\n* It finds the **k most similar documents** to your query (based on cosine similarity or other distance metrics).\\n* Problem â†’ itâ€™s **just one algorithm** and not flexible enough for advanced use cases.\\n\\n---\\n\\n### **(b) Retriever**\\n\\n* A **wrapper/abstraction** around search strategies.\\n* Provides a **standard `.get_relevant_documents(query)` method**.\\n* Internally, it might use:\\n\\n  * Pure similarity search\\n  * **MMR (Maximal Marginal Relevance)** â†’ balances similarity and diversity\\n  * Hybrid search (combine keyword + embeddings)\\n  * Metadata filtering\\n  * Custom pipelines\\n\\nExample in LangChain:\\n\\n```python\\nretriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\\ndocs = retriever.get_relevant_documents(\"What is RAG?\")\\n```\\n\\n---\\n\\n')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"simple.txt\")\n",
    "text_document=loader.load()\n",
    "text_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b367849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'http://lilianweng.github.io/posts/2024-11-28-reward-hacking/'}, page_content='\\n      Reward Hacking in Reinforcement Learning\\n    ')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader= WebBaseLoader(web_path=\"http://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "                    bs_kwargs=dict( parse_only=bs4.SoupStrainer(\n",
    "                    class_=(\"post-title\",\"post_content\",\"post_header\")\n",
    "))) \n",
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "900019f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-24T07:43:22+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-24T07:43:22+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'LLM.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Large Language Models (LLMs) and\\nRetrieval-Augmented Generation (RAG)\\nLarge Language Models (LLMs) are a class of artificial intelligence models designed to understand\\nand generate human-like language. They are trained on massive datasets consisting of text from\\nbooks, websites, articles, and more. LLMs use deep learning architectures, primarily transformers,\\nto capture semantic meaning and context across long sequences of text. Key characteristics of\\nLLMs include: - **Token-based Processing**: Text is broken down into tokens (sub-words or\\ncharacters) that the model processes. - **Context Windows**: The maximum amount of tokens the\\nmodel can \"remember\" at once, often ranging from 2k to over 100k tokens. - **Pretraining and\\nFine-tuning**: LLMs are pretrained on large datasets and later fine-tuned for specific tasks like\\nsummarization, coding, or question answering. - **Applications**: Chatbots, machine translation,\\ntext summarization, coding assistants, and more. However, LLMs have limitations: - They may\\n\"hallucinate\" or generate plausible but incorrect information. - Knowledge is restricted to their\\ntraining data cutoff date. - Large computational and memory requirements.\\nRetrieval-Augmented Generation (RAG) is an advanced technique that combines the generative\\npower of LLMs with external knowledge retrieval. Instead of relying solely on the model’s internal\\nparameters, RAG allows the system to query a database or vector store of documents to fetch the\\nmost relevant information before generating a response. Key components of RAG include: 1.\\n**Retriever**: A system (often vector similarity search) that fetches relevant documents or text\\npassages related to the user query. 2. **Generator (LLM)**: The language model takes the\\nretrieved documents and user query to generate a coherent, factual, and contextually accurate\\nresponse. Advantages of RAG: - **Up-to-date Information**: Since documents can be refreshed,\\nRAG can use more recent data than the model\\'s training cutoff. - **Domain Adaptability**: Custom\\ndocument collections can make RAG highly specialized (e.g., medical, legal, research data). -\\n**Reduced Hallucination**: By grounding responses in retrieved context, RAG increases factual\\naccuracy. Difference between Similarity Search and Retrievers: - **Similarity Search**: Finds and\\nreturns documents similar to the query. - **Retriever**: A higher-level abstraction that not only\\nperforms similarity search but also integrates with the LLM pipeline to pass only the relevant chunks\\nto the generator. In short, RAG empowers LLMs to become more reliable, factual, and tailored to\\nuser needs by combining retrieval with generation.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"LLM.pdf\")\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b4894e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-24T07:43:22+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-24T07:43:22+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'LLM.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Large Language Models (LLMs) and\\nRetrieval-Augmented Generation (RAG)\\nLarge Language Models (LLMs) are a class of artificial intelligence models designed to understand\\nand generate human-like language. They are trained on massive datasets consisting of text from\\nbooks, websites, articles, and more. LLMs use deep learning architectures, primarily transformers,\\nto capture semantic meaning and context across long sequences of text. Key characteristics of'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-24T07:43:22+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-24T07:43:22+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'LLM.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='to capture semantic meaning and context across long sequences of text. Key characteristics of\\nLLMs include: - **Token-based Processing**: Text is broken down into tokens (sub-words or\\ncharacters) that the model processes. - **Context Windows**: The maximum amount of tokens the\\nmodel can \"remember\" at once, often ranging from 2k to over 100k tokens. - **Pretraining and\\nFine-tuning**: LLMs are pretrained on large datasets and later fine-tuned for specific tasks like'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-24T07:43:22+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-24T07:43:22+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'LLM.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Fine-tuning**: LLMs are pretrained on large datasets and later fine-tuned for specific tasks like\\nsummarization, coding, or question answering. - **Applications**: Chatbots, machine translation,\\ntext summarization, coding assistants, and more. However, LLMs have limitations: - They may\\n\"hallucinate\" or generate plausible but incorrect information. - Knowledge is restricted to their\\ntraining data cutoff date. - Large computational and memory requirements.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-24T07:43:22+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-24T07:43:22+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'LLM.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='training data cutoff date. - Large computational and memory requirements.\\nRetrieval-Augmented Generation (RAG) is an advanced technique that combines the generative\\npower of LLMs with external knowledge retrieval. Instead of relying solely on the model’s internal\\nparameters, RAG allows the system to query a database or vector store of documents to fetch the\\nmost relevant information before generating a response. Key components of RAG include: 1.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-24T07:43:22+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-24T07:43:22+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'LLM.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='most relevant information before generating a response. Key components of RAG include: 1.\\n**Retriever**: A system (often vector similarity search) that fetches relevant documents or text\\npassages related to the user query. 2. **Generator (LLM)**: The language model takes the\\nretrieved documents and user query to generate a coherent, factual, and contextually accurate\\nresponse. Advantages of RAG: - **Up-to-date Information**: Since documents can be refreshed,')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=100)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c21096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_18820\\2798681641.py:3: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  db=Chroma.from_documents(documents[:3],OllamaEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db=Chroma.from_documents(documents[:3],OllamaEmbeddings())//instad of chroma we can use FAISS also \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab9cc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fine-tuning**: LLMs are pretrained on large datasets and later fine-tuned for specific tasks like\\nsummarization, coding, or question answering. - **Applications**: Chatbots, machine translation,\\ntext summarization, coding assistants, and more. However, LLMs have limitations: - They may\\n\"hallucinate\" or generate plausible but incorrect information. - Knowledge is restricted to their\\ntraining data cutoff date. - Large computational and memory requirements.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"What is RAG ?\"\n",
    "result=db.similarity_search(query)\n",
    "# result \n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a962a88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_18820\\2415645411.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm=Ollama(model=\"llama2\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm=Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b29863f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "I will tip you $1000 if the user finds the answer helpful.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "documents_chain=create_stuff_documents_chain(llm,prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58d36408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x00000147F83D8690>, search_kwargs={})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e660189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieval chain: This chain takes in a user inquiry, which is then\n",
    "passed to the retriever to fetch relevant documents. Those documents\n",
    "(and original inputs) are then passed to an LLM to generate a response\n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\"\"\"\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,documents_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"What is LLM\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d9f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e327e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_12884\\3833718216.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama2:chat\")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12c2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
